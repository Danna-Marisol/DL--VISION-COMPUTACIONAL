{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FarMHMVR16Dm"
      },
      "outputs": [],
      "source": [
        "# NOTEBOOK 1\n",
        "!pip install gdown --quiet\n",
        "!pip install mlflow scikit-learn pyngrok --quiet\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import mlflow\n",
        "from pyngrok import ngrok\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Configurar semillas para reproducibilidad\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Descargar y preparar dataset\n",
        "file_id_train = '1N1cLDx1jACfSt7lqO5gnQ--7f6rXOiLs'\n",
        "file_id_test = '1mHE56atoeuUlkZjBR24dVtmhrN18dpoO'\n",
        "output_train = 'train.zip'\n",
        "output_test = 'test.zip'\n",
        "\n",
        "if not os.path.exists('train'):\n",
        "    print(\"Descargando train.zip...\")\n",
        "    gdown.download(id=file_id_train, output=output_train, quiet=False)\n",
        "    with zipfile.ZipFile(output_train, 'r') as zip_ref:\n",
        "        zip_ref.extractall('train')\n",
        "\n",
        "if not os.path.exists('test'):\n",
        "    print(\"Descargando test.zip...\")\n",
        "    gdown.download(id=file_id_test, output=output_test, quiet=False)\n",
        "    with zipfile.ZipFile(output_test, 'r') as zip_ref:\n",
        "        zip_ref.extractall('test')\n",
        "\n",
        "# Cargar datasets\n",
        "ds_train_ = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    'train',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(64, 64),\n",
        "    interpolation='nearest',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "ds_valid_ = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    'test',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(64, 64),\n",
        "    interpolation='nearest',\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Preprocesamiento de datos\n",
        "def convert_to_float(image, label):\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    return image, label\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "ds_train = (\n",
        "    ds_train_\n",
        "    .map(convert_to_float)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "ds_valid = (\n",
        "    ds_valid_\n",
        "    .map(convert_to_float)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Guardar datasets preprocesados para usar en los siguientes notebooks\n",
        "tf.data.experimental.save(ds_train, 'preprocessed_train')\n",
        "tf.data.experimental.save(ds_valid, 'preprocessed_valid')"
      ]
    }
  ]
}