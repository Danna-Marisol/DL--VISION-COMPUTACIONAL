{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ocj62wu2JKn"
      },
      "outputs": [],
      "source": [
        "# NOTEBOOK 2\n",
        "import os\n",
        "import mlflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Cargar datasets preprocesados\n",
        "ds_train = tf.data.experimental.load('preprocessed_train')\n",
        "ds_valid = tf.data.experimental.load('preprocessed_valid')\n",
        "\n",
        "# Ciclo de entrenamiento\n",
        "N_RUNS = 5\n",
        "accuracies = []\n",
        "f1_scores = []\n",
        "best_val_acc = 0\n",
        "best_history = None\n",
        "mlflow.set_experiment(\"clasificacion_perros_y_gatos\")\n",
        "\n",
        "for run in range(N_RUNS):\n",
        "    print(f\"\\nEjecutando corrida {run+1}/{N_RUNS}\")\n",
        "\n",
        "    # Definir modelo con data augmentation\n",
        "    data_augmentation = keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\", seed=42),\n",
        "        layers.RandomRotation(0.3, seed=42),\n",
        "        layers.RandomZoom(0.2, seed=42),\n",
        "        layers.RandomContrast(0.2, seed=42),\n",
        "    ])\n",
        "\n",
        "    # Definir modelo optimizado\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        data_augmentation,\n",
        "        layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPool2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPool2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPool2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    # Compilar modelo\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "\n",
        "    checkpoint_cb = ModelCheckpoint(\"mejor_modelo.keras\", save_best_only=True, monitor='val_binary_accuracy', mode='max')\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        min_delta=0.001,\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        ds_train,\n",
        "        validation_data=ds_valid,\n",
        "        epochs=30,\n",
        "        callbacks=[early_stopping, checkpoint_cb],\n",
        "    )\n",
        "\n",
        "    # Evaluar métricas\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for images, labels in ds_valid:\n",
        "        preds = model.predict(images, verbose=0)\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred.extend((preds > 0.5).astype(int).flatten())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    if acc > best_val_acc:\n",
        "        best_val_acc = acc\n",
        "        best_history = history.history\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"run_{run+1}\"):\n",
        "        mlflow.log_param(\"optimizer\", \"Adam\")\n",
        "        mlflow.log_param(\"loss\", \"binary_crossentropy\")\n",
        "        mlflow.log_param(\"batch_size\", 64)\n",
        "        mlflow.log_param(\"image_size\", (64, 64))\n",
        "        mlflow.log_param(\"run\", run + 1)\n",
        "        mlflow.log_param(\"random_seed\", 42)\n",
        "        mlflow.log_metric(\"val_accuracy\", acc)\n",
        "        mlflow.log_metric(\"val_f1_score\", f1)\n",
        "        mlflow.tensorflow.log_model(model, f\"modelo_cnn_run_{run+1}\")\n",
        "\n",
        "# Guardar métricas para el siguiente notebook\n",
        "import pickle\n",
        "with open('metrics.pkl', 'wb') as f:\n",
        "    pickle.dump({'accuracies': accuracies, 'f1_scores': f1_scores, 'best_val_acc': best_val_acc, 'best_history': best_history}, f)"
      ]
    }
  ]
}